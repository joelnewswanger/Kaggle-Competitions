{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas\n",
    "import numpy\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Average, Input\n",
    "from tqdm import tqdm\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from random import randint\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = '/home/joel/Documents/KaggleComps/SeedlingId/'\n",
    "trainDir = os.path.join(dataDir, 'train')\n",
    "valDir = os.path.join(dataDir, 'validate')\n",
    "testDir = os.path.join(dataDir, 'test')\n",
    "sampleSub = pandas.read_csv(os.path.join(dataDir, 'sample_submission.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent',\n",
    "              'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet']\n",
    "weeds = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Fat Hen', 'Loose Silky-bent',\n",
    "         'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill']\n",
    "crops = ['Common wheat', 'Maize', 'Sugar beet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the samples into category folders as expected by flow_from_directory()\n",
    "minNumSamples = 221\n",
    "trainVal = []\n",
    "for cat in os.listdir(os.path.join(dataDir,'trainVal')):\n",
    "    for file in os.listdir(os.path.join(dataDir,'trainVal', cat)):\n",
    "        if randint(1,100) < 80:\n",
    "            os.renames(os.path.join(dataDir,'trainVal',cat,file),os.path.join(dataDir,'train',cat,file))\n",
    "        else:\n",
    "            os.renames(os.path.join(dataDir,'trainVal',cat,file),os.path.join(dataDir,'validate',cat,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 176 pictures of Black-grass in the train folder\n",
      "There are 45 pictures of Black-grass in the validate folder\n",
      "There are 164 pictures of Charlock in the train folder\n",
      "There are 57 pictures of Charlock in the validate folder\n",
      "There are 169 pictures of Cleavers in the train folder\n",
      "There are 52 pictures of Cleavers in the validate folder\n",
      "There are 175 pictures of Common Chickweed in the train folder\n",
      "There are 46 pictures of Common Chickweed in the validate folder\n",
      "There are 182 pictures of Common wheat in the train folder\n",
      "There are 39 pictures of Common wheat in the validate folder\n",
      "There are 174 pictures of Fat Hen in the train folder\n",
      "There are 47 pictures of Fat Hen in the validate folder\n",
      "There are 179 pictures of Loose Silky-bent in the train folder\n",
      "There are 42 pictures of Loose Silky-bent in the validate folder\n",
      "There are 170 pictures of Maize in the train folder\n",
      "There are 51 pictures of Maize in the validate folder\n",
      "There are 166 pictures of Scentless Mayweed in the train folder\n",
      "There are 55 pictures of Scentless Mayweed in the validate folder\n",
      "There are 167 pictures of Shepherds Purse in the train folder\n",
      "There are 54 pictures of Shepherds Purse in the validate folder\n",
      "There are 178 pictures of Small-flowered Cranesbill in the train folder\n",
      "There are 43 pictures of Small-flowered Cranesbill in the validate folder\n",
      "There are 167 pictures of Sugar beet in the train folder\n",
      "There are 54 pictures of Sugar beet in the validate folder\n",
      "There are 2067 training images and 585 validation images\n"
     ]
    }
   ],
   "source": [
    "# Stats\n",
    "T = 0\n",
    "V = 0\n",
    "for cat in categories:\n",
    "    tnum = len(os.listdir(os.path.join(dataDir,'train', cat)))\n",
    "    vnum = len(os.listdir(os.path.join(dataDir,'validate', cat)))\n",
    "    print('There are {} pictures of {} in the train folder'.format(tnum, cat))\n",
    "    print('There are {} pictures of {} in the validate folder'.format(vnum, cat))\n",
    "    T+=tnum\n",
    "    V+=vnum\n",
    "print(\"There are {} training images and {} validation images\".format(T, V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing on the fly.\n",
    "trainGenerator=image.ImageDataGenerator(\n",
    "        rescale=1/255,\n",
    "        shear_range=0.2, zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "testGenerator=image.ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2067 images belonging to 12 classes.\n",
      "Found 585 images belonging to 12 classes.\n",
      "Epoch 1/20\n",
      "103/104 [============================>.] - ETA: 0s - loss: 2.3588 - acc: 0.1727Epoch 00001: val_loss improved from inf to 2.20130, saving model to weights/baselineWithAugmentation200.hdf5\n",
      "104/104 [==============================] - 116s 1s/step - loss: 2.3566 - acc: 0.1720 - val_loss: 2.2013 - val_acc: 0.2393\n",
      "Epoch 2/20\n",
      "103/104 [============================>.] - ETA: 0s - loss: 1.7387 - acc: 0.3693Epoch 00002: val_loss did not improve\n",
      "104/104 [==============================] - 117s 1s/step - loss: 1.7398 - acc: 0.3711 - val_loss: 2.2202 - val_acc: 0.2291\n",
      "Epoch 3/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 1.4494 - acc: 0.4902Epoch 00003: val_loss improved from 2.20130 to 1.18226, saving model to weights/baselineWithAugmentation200.hdf5\n",
      "104/104 [==============================] - 121s 1s/step - loss: 1.4465 - acc: 0.4913 - val_loss: 1.1823 - val_acc: 0.6137\n",
      "Epoch 4/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 1.2172 - acc: 0.5963Epoch 00004: val_loss did not improve\n",
      "104/104 [==============================] - 121s 1s/step - loss: 1.2235 - acc: 0.5954 - val_loss: 1.2985 - val_acc: 0.5812\n",
      "Epoch 5/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 1.1014 - acc: 0.6264Epoch 00005: val_loss improved from 1.18226 to 1.15120, saving model to weights/baselineWithAugmentation200.hdf5\n",
      "104/104 [==============================] - 118s 1s/step - loss: 1.1008 - acc: 0.6267 - val_loss: 1.1512 - val_acc: 0.6479\n",
      "Epoch 6/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.9740 - acc: 0.6794Epoch 00006: val_loss improved from 1.15120 to 0.93379, saving model to weights/baselineWithAugmentation200.hdf5\n",
      "104/104 [==============================] - 119s 1s/step - loss: 0.9749 - acc: 0.6787 - val_loss: 0.9338 - val_acc: 0.7094\n",
      "Epoch 7/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.8886 - acc: 0.7220Epoch 00007: val_loss improved from 0.93379 to 0.84916, saving model to weights/baselineWithAugmentation200.hdf5\n",
      "104/104 [==============================] - 119s 1s/step - loss: 0.8887 - acc: 0.7227 - val_loss: 0.8492 - val_acc: 0.7350\n",
      "Epoch 8/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.8043 - acc: 0.7374Epoch 00008: val_loss did not improve\n",
      "104/104 [==============================] - 122s 1s/step - loss: 0.8030 - acc: 0.7376 - val_loss: 0.8873 - val_acc: 0.6957\n",
      "Epoch 9/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.7523 - acc: 0.7501Epoch 00009: val_loss did not improve\n",
      "104/104 [==============================] - 117s 1s/step - loss: 0.7510 - acc: 0.7505 - val_loss: 0.8913 - val_acc: 0.7043\n",
      "Epoch 10/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.7211 - acc: 0.7695Epoch 00010: val_loss improved from 0.84916 to 0.80510, saving model to weights/baselineWithAugmentation200.hdf5\n",
      "104/104 [==============================] - 118s 1s/step - loss: 0.7180 - acc: 0.7712 - val_loss: 0.8051 - val_acc: 0.7590\n",
      "Epoch 11/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.6468 - acc: 0.7870Epoch 00011: val_loss did not improve\n",
      "104/104 [==============================] - 118s 1s/step - loss: 0.6434 - acc: 0.7880 - val_loss: 0.8075 - val_acc: 0.7368\n",
      "Epoch 12/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.6079 - acc: 0.7958Epoch 00012: val_loss improved from 0.80510 to 0.78119, saving model to weights/baselineWithAugmentation200.hdf5\n",
      "104/104 [==============================] - 119s 1s/step - loss: 0.6089 - acc: 0.7944 - val_loss: 0.7812 - val_acc: 0.7470\n",
      "Epoch 13/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.5557 - acc: 0.8141Epoch 00013: val_loss improved from 0.78119 to 0.69389, saving model to weights/baselineWithAugmentation200.hdf5\n",
      "104/104 [==============================] - 120s 1s/step - loss: 0.5566 - acc: 0.8135 - val_loss: 0.6939 - val_acc: 0.7692\n",
      "Epoch 14/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.5349 - acc: 0.8301Epoch 00014: val_loss did not improve\n",
      "104/104 [==============================] - 119s 1s/step - loss: 0.5356 - acc: 0.8303 - val_loss: 0.7920 - val_acc: 0.7470\n",
      "Epoch 15/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.5052 - acc: 0.8274Epoch 00015: val_loss did not improve\n",
      "104/104 [==============================] - 121s 1s/step - loss: 0.5054 - acc: 0.8267 - val_loss: 0.8092 - val_acc: 0.7436\n",
      "Epoch 16/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.4617 - acc: 0.8399Epoch 00016: val_loss did not improve\n",
      "104/104 [==============================] - 119s 1s/step - loss: 0.4618 - acc: 0.8395 - val_loss: 0.7431 - val_acc: 0.7538\n",
      "Epoch 17/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.4471 - acc: 0.8529Epoch 00017: val_loss did not improve\n",
      "104/104 [==============================] - 121s 1s/step - loss: 0.4473 - acc: 0.8524 - val_loss: 0.7264 - val_acc: 0.7538\n",
      "Epoch 18/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.4419 - acc: 0.8578Epoch 00018: val_loss did not improve\n",
      "104/104 [==============================] - 122s 1s/step - loss: 0.4420 - acc: 0.8578 - val_loss: 0.7049 - val_acc: 0.7658\n",
      "Epoch 19/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.4064 - acc: 0.8657Epoch 00019: val_loss did not improve\n",
      "104/104 [==============================] - 118s 1s/step - loss: 0.4032 - acc: 0.8671 - val_loss: 0.7133 - val_acc: 0.8051\n",
      "Epoch 20/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.3882 - acc: 0.8753Epoch 00020: val_loss did not improve\n",
      "104/104 [==============================] - 121s 1s/step - loss: 0.3874 - acc: 0.8756 - val_loss: 0.7623 - val_acc: 0.7675\n",
      "Found 2067 images belonging to 12 classes.\n",
      "Found 585 images belonging to 12 classes.\n",
      "Epoch 1/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 2.2340 - acc: 0.2252Epoch 00001: val_loss improved from inf to 1.71814, saving model to weights/baselineWithAugmentation250.hdf5\n",
      "104/104 [==============================] - 186s 2s/step - loss: 2.2287 - acc: 0.2269 - val_loss: 1.7181 - val_acc: 0.3829\n",
      "Epoch 2/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 1.7024 - acc: 0.3955Epoch 00002: val_loss improved from 1.71814 to 1.37590, saving model to weights/baselineWithAugmentation250.hdf5\n",
      "104/104 [==============================] - 183s 2s/step - loss: 1.6974 - acc: 0.3994 - val_loss: 1.3759 - val_acc: 0.5128\n",
      "Epoch 3/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 1.3906 - acc: 0.5132Epoch 00003: val_loss improved from 1.37590 to 1.23764, saving model to weights/baselineWithAugmentation250.hdf5\n",
      "104/104 [==============================] - 185s 2s/step - loss: 1.3878 - acc: 0.5136 - val_loss: 1.2376 - val_acc: 0.5556\n",
      "Epoch 4/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 1.2332 - acc: 0.5883Epoch 00004: val_loss did not improve\n",
      "104/104 [==============================] - 181s 2s/step - loss: 1.2312 - acc: 0.5894 - val_loss: 1.4529 - val_acc: 0.5385\n",
      "Epoch 5/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 1.0910 - acc: 0.6415Epoch 00005: val_loss improved from 1.23764 to 1.04395, saving model to weights/baselineWithAugmentation250.hdf5\n",
      "104/104 [==============================] - 180s 2s/step - loss: 1.0936 - acc: 0.6416 - val_loss: 1.0440 - val_acc: 0.6581\n",
      "Epoch 6/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.9586 - acc: 0.6769Epoch 00006: val_loss improved from 1.04395 to 1.01304, saving model to weights/baselineWithAugmentation250.hdf5\n",
      "104/104 [==============================] - 182s 2s/step - loss: 0.9632 - acc: 0.6752 - val_loss: 1.0130 - val_acc: 0.6855\n",
      "Epoch 7/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.8745 - acc: 0.7031Epoch 00007: val_loss improved from 1.01304 to 0.95435, saving model to weights/baselineWithAugmentation250.hdf5\n",
      "104/104 [==============================] - 183s 2s/step - loss: 0.8727 - acc: 0.7025 - val_loss: 0.9544 - val_acc: 0.6838\n",
      "Epoch 8/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.8088 - acc: 0.7424Epoch 00008: val_loss improved from 0.95435 to 0.88478, saving model to weights/baselineWithAugmentation250.hdf5\n",
      "104/104 [==============================] - 182s 2s/step - loss: 0.8056 - acc: 0.7439 - val_loss: 0.8848 - val_acc: 0.7026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.7126 - acc: 0.7666Epoch 00009: val_loss did not improve\n",
      "104/104 [==============================] - 184s 2s/step - loss: 0.7154 - acc: 0.7665 - val_loss: 0.9459 - val_acc: 0.6889\n",
      "Epoch 10/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.6766 - acc: 0.7745Epoch 00010: val_loss improved from 0.88478 to 0.72554, saving model to weights/baselineWithAugmentation250.hdf5\n",
      "104/104 [==============================] - 180s 2s/step - loss: 0.6738 - acc: 0.7762 - val_loss: 0.7255 - val_acc: 0.7590\n",
      "Epoch 11/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.6340 - acc: 0.7962Epoch 00011: val_loss improved from 0.72554 to 0.70610, saving model to weights/baselineWithAugmentation250.hdf5\n",
      "104/104 [==============================] - 182s 2s/step - loss: 0.6312 - acc: 0.7972 - val_loss: 0.7061 - val_acc: 0.7607\n",
      "Epoch 12/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.5454 - acc: 0.8267Epoch 00012: val_loss did not improve\n",
      "104/104 [==============================] - 181s 2s/step - loss: 0.5435 - acc: 0.8279 - val_loss: 0.7292 - val_acc: 0.7316\n",
      "Epoch 13/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.5513 - acc: 0.8244Epoch 00013: val_loss did not improve\n",
      "104/104 [==============================] - 181s 2s/step - loss: 0.5497 - acc: 0.8251 - val_loss: 0.8244 - val_acc: 0.7333\n",
      "Epoch 14/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.5002 - acc: 0.8296Epoch 00014: val_loss did not improve\n",
      "104/104 [==============================] - 178s 2s/step - loss: 0.4977 - acc: 0.8303 - val_loss: 0.7307 - val_acc: 0.7573\n",
      "Epoch 15/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.4539 - acc: 0.8496Epoch 00015: val_loss did not improve\n",
      "104/104 [==============================] - 181s 2s/step - loss: 0.4533 - acc: 0.8496 - val_loss: 0.7753 - val_acc: 0.7590\n",
      "Epoch 16/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.4342 - acc: 0.8511Epoch 00016: val_loss did not improve\n",
      "104/104 [==============================] - 177s 2s/step - loss: 0.4343 - acc: 0.8511 - val_loss: 0.7785 - val_acc: 0.7624\n",
      "Epoch 17/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.4146 - acc: 0.8656Epoch 00017: val_loss improved from 0.70610 to 0.57617, saving model to weights/baselineWithAugmentation250.hdf5\n",
      "104/104 [==============================] - 181s 2s/step - loss: 0.4142 - acc: 0.8659 - val_loss: 0.5762 - val_acc: 0.8222\n",
      "Epoch 18/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.3796 - acc: 0.8676Epoch 00018: val_loss did not improve\n",
      "104/104 [==============================] - 181s 2s/step - loss: 0.3788 - acc: 0.8684 - val_loss: 0.6957 - val_acc: 0.7675\n",
      "Epoch 19/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.3653 - acc: 0.8807Epoch 00019: val_loss did not improve\n",
      "104/104 [==============================] - 181s 2s/step - loss: 0.3631 - acc: 0.8818 - val_loss: 0.6882 - val_acc: 0.7897\n",
      "Epoch 20/20\n",
      "103/104 [============================>.] - ETA: 1s - loss: 0.3723 - acc: 0.8767Epoch 00020: val_loss did not improve\n",
      "104/104 [==============================] - 181s 2s/step - loss: 0.3700 - acc: 0.8774 - val_loss: 0.5909 - val_acc: 0.8205\n",
      "Found 2067 images belonging to 12 classes.\n",
      "Found 585 images belonging to 12 classes.\n",
      "Epoch 1/20\n",
      "103/104 [============================>.] - ETA: 2s - loss: 2.4890 - acc: 0.1645Epoch 00001: val_loss improved from inf to 1.93872, saving model to weights/baselineWithAugmentation300.hdf5\n",
      "104/104 [==============================] - 258s 2s/step - loss: 2.4850 - acc: 0.1639 - val_loss: 1.9387 - val_acc: 0.3111\n",
      "Epoch 2/20\n",
      "103/104 [============================>.] - ETA: 2s - loss: 1.8548 - acc: 0.3571Epoch 00002: val_loss improved from 1.93872 to 1.86817, saving model to weights/baselineWithAugmentation300.hdf5\n",
      "104/104 [==============================] - 256s 2s/step - loss: 1.8513 - acc: 0.3590 - val_loss: 1.8682 - val_acc: 0.3368\n",
      "Epoch 3/20\n",
      "103/104 [============================>.] - ETA: 2s - loss: 1.4450 - acc: 0.5017Epoch 00003: val_loss improved from 1.86817 to 1.32958, saving model to weights/baselineWithAugmentation300.hdf5\n",
      "104/104 [==============================] - 256s 2s/step - loss: 1.4448 - acc: 0.5016 - val_loss: 1.3296 - val_acc: 0.5538\n",
      "Epoch 4/20\n",
      "103/104 [============================>.] - ETA: 2s - loss: 1.2091 - acc: 0.6026Epoch 00004: val_loss improved from 1.32958 to 1.05055, saving model to weights/baselineWithAugmentation300.hdf5\n",
      "104/104 [==============================] - 255s 2s/step - loss: 1.2046 - acc: 0.6050 - val_loss: 1.0506 - val_acc: 0.6513\n",
      "Epoch 5/20\n",
      "103/104 [============================>.] - ETA: 2s - loss: 1.0898 - acc: 0.6309Epoch 00005: val_loss improved from 1.05055 to 0.98634, saving model to weights/baselineWithAugmentation300.hdf5\n",
      "104/104 [==============================] - 255s 2s/step - loss: 1.0893 - acc: 0.6311 - val_loss: 0.9863 - val_acc: 0.6701\n",
      "Epoch 6/20\n",
      "103/104 [============================>.] - ETA: 2s - loss: 0.9319 - acc: 0.6963Epoch 00006: val_loss did not improve\n",
      "104/104 [==============================] - 256s 2s/step - loss: 0.9294 - acc: 0.6973 - val_loss: 1.0362 - val_acc: 0.6581\n",
      "Epoch 7/20\n",
      "103/104 [============================>.] - ETA: 2s - loss: 0.8296 - acc: 0.7260Epoch 00007: val_loss improved from 0.98634 to 0.81377, saving model to weights/baselineWithAugmentation300.hdf5\n",
      "104/104 [==============================] - 258s 2s/step - loss: 0.8256 - acc: 0.7272 - val_loss: 0.8138 - val_acc: 0.7333\n",
      "Epoch 8/20\n",
      "103/104 [============================>.] - ETA: 2s - loss: 0.7439 - acc: 0.7617Epoch 00008: val_loss did not improve\n",
      "104/104 [==============================] - 256s 2s/step - loss: 0.7456 - acc: 0.7606 - val_loss: 0.8312 - val_acc: 0.7299\n",
      "Epoch 9/20\n",
      "103/104 [============================>.] - ETA: 2s - loss: 0.6903 - acc: 0.7779Epoch 00009: val_loss improved from 0.81377 to 0.69274, saving model to weights/baselineWithAugmentation300.hdf5\n",
      "104/104 [==============================] - 255s 2s/step - loss: 0.6865 - acc: 0.7801 - val_loss: 0.6927 - val_acc: 0.7675\n",
      "Epoch 10/20\n",
      "103/104 [============================>.] - ETA: 2s - loss: 0.6318 - acc: 0.7985Epoch 00010: val_loss did not improve\n",
      "104/104 [==============================] - 255s 2s/step - loss: 0.6302 - acc: 0.7990 - val_loss: 0.8184 - val_acc: 0.7385\n",
      "Epoch 11/20\n",
      "103/104 [============================>.] - ETA: 2s - loss: 0.5963 - acc: 0.8035Epoch 00011: val_loss did not improve\n",
      "104/104 [==============================] - 256s 2s/step - loss: 0.5943 - acc: 0.8040 - val_loss: 0.7614 - val_acc: 0.7556\n",
      "Epoch 12/20\n",
      "103/104 [============================>.] - ETA: 2s - loss: 0.5493 - acc: 0.8142Epoch 00012: val_loss improved from 0.69274 to 0.60626, saving model to weights/baselineWithAugmentation300.hdf5\n",
      "104/104 [==============================] - 256s 2s/step - loss: 0.5466 - acc: 0.8150 - val_loss: 0.6063 - val_acc: 0.7966\n",
      "Epoch 13/20\n",
      "103/104 [============================>.] - ETA: 2s - loss: 0.5017 - acc: 0.8438Epoch 00013: val_loss did not improve\n",
      "104/104 [==============================] - 256s 2s/step - loss: 0.5036 - acc: 0.8429 - val_loss: 0.6668 - val_acc: 0.7915\n",
      "Epoch 14/20\n",
      "103/104 [============================>.] - ETA: 2s - loss: 0.4542 - acc: 0.8496Epoch 00014: val_loss did not improve\n",
      "104/104 [==============================] - 256s 2s/step - loss: 0.4525 - acc: 0.8496 - val_loss: 0.8241 - val_acc: 0.7624\n",
      "Epoch 15/20\n",
      "103/104 [============================>.] - ETA: 2s - loss: 0.4376 - acc: 0.8617Epoch 00015: val_loss improved from 0.60626 to 0.58326, saving model to weights/baselineWithAugmentation300.hdf5\n",
      "104/104 [==============================] - 258s 2s/step - loss: 0.4372 - acc: 0.8610 - val_loss: 0.5833 - val_acc: 0.7949\n",
      "Epoch 16/20\n",
      "103/104 [============================>.] - ETA: 2s - loss: 0.4463 - acc: 0.8684Epoch 00016: val_loss did not improve\n",
      "104/104 [==============================] - 255s 2s/step - loss: 0.4507 - acc: 0.8683 - val_loss: 0.9152 - val_acc: 0.7385\n",
      "Epoch 17/20\n",
      "103/104 [============================>.] - ETA: 2s - loss: 0.4383 - acc: 0.8628Epoch 00017: val_loss did not improve\n",
      "104/104 [==============================] - 256s 2s/step - loss: 0.4407 - acc: 0.8618 - val_loss: 0.6351 - val_acc: 0.8000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20\n",
      "103/104 [============================>.] - ETA: 2s - loss: 0.3518 - acc: 0.8889Epoch 00018: val_loss did not improve\n",
      "104/104 [==============================] - 254s 2s/step - loss: 0.3533 - acc: 0.8881 - val_loss: 0.6462 - val_acc: 0.7949\n",
      "Epoch 19/20\n",
      "103/104 [============================>.] - ETA: 2s - loss: 0.3656 - acc: 0.8883Epoch 00019: val_loss did not improve\n",
      "104/104 [==============================] - 255s 2s/step - loss: 0.3630 - acc: 0.8889 - val_loss: 0.7691 - val_acc: 0.7863\n",
      "Epoch 20/20\n",
      "103/104 [============================>.] - ETA: 2s - loss: 0.3332 - acc: 0.8879Epoch 00020: val_loss did not improve\n",
      "104/104 [==============================] - 254s 2s/step - loss: 0.3387 - acc: 0.8865 - val_loss: 0.6787 - val_acc: 0.8171\n",
      "Found 2067 images belonging to 12 classes.\n",
      "Found 585 images belonging to 12 classes.\n",
      "Epoch 1/20\n",
      "103/104 [============================>.] - ETA: 3s - loss: 2.5918 - acc: 0.1947Epoch 00001: val_loss improved from inf to 1.86002, saving model to weights/baselineWithAugmentation400.hdf5\n",
      "104/104 [==============================] - 450s 4s/step - loss: 2.5859 - acc: 0.1947 - val_loss: 1.8600 - val_acc: 0.2872\n",
      "Epoch 2/20\n",
      "103/104 [============================>.] - ETA: 3s - loss: 1.6998 - acc: 0.3867Epoch 00002: val_loss improved from 1.86002 to 1.62026, saving model to weights/baselineWithAugmentation400.hdf5\n",
      "104/104 [==============================] - 445s 4s/step - loss: 1.6998 - acc: 0.3854 - val_loss: 1.6203 - val_acc: 0.4444\n",
      "Epoch 3/20\n",
      "103/104 [============================>.] - ETA: 3s - loss: 1.4333 - acc: 0.4944Epoch 00003: val_loss improved from 1.62026 to 1.21934, saving model to weights/baselineWithAugmentation400.hdf5\n",
      "104/104 [==============================] - 444s 4s/step - loss: 1.4326 - acc: 0.4944 - val_loss: 1.2193 - val_acc: 0.5932\n",
      "Epoch 4/20\n",
      "103/104 [============================>.] - ETA: 3s - loss: 1.2149 - acc: 0.5920Epoch 00004: val_loss did not improve\n",
      "104/104 [==============================] - 444s 4s/step - loss: 1.2174 - acc: 0.5906 - val_loss: 1.2351 - val_acc: 0.5692\n",
      "Epoch 5/20\n",
      "103/104 [============================>.] - ETA: 3s - loss: 1.1175 - acc: 0.6215Epoch 00005: val_loss did not improve\n",
      "104/104 [==============================] - 445s 4s/step - loss: 1.1128 - acc: 0.6227 - val_loss: 1.3110 - val_acc: 0.5829\n",
      "Epoch 6/20\n",
      "103/104 [============================>.] - ETA: 3s - loss: 0.9827 - acc: 0.6773Epoch 00006: val_loss improved from 1.21934 to 0.98253, saving model to weights/baselineWithAugmentation400.hdf5\n",
      "104/104 [==============================] - 443s 4s/step - loss: 0.9809 - acc: 0.6775 - val_loss: 0.9825 - val_acc: 0.6684\n",
      "Epoch 7/20\n",
      "103/104 [============================>.] - ETA: 3s - loss: 0.8765 - acc: 0.7123Epoch 00007: val_loss did not improve\n",
      "104/104 [==============================] - 444s 4s/step - loss: 0.8768 - acc: 0.7127 - val_loss: 1.0231 - val_acc: 0.6650\n",
      "Epoch 8/20\n",
      "103/104 [============================>.] - ETA: 3s - loss: 0.8137 - acc: 0.7461Epoch 00008: val_loss did not improve\n",
      "104/104 [==============================] - 443s 4s/step - loss: 0.8195 - acc: 0.7437 - val_loss: 1.1329 - val_acc: 0.6598\n",
      "Epoch 9/20\n",
      "103/104 [============================>.] - ETA: 3s - loss: 0.7241 - acc: 0.7743Epoch 00009: val_loss did not improve\n",
      "104/104 [==============================] - 445s 4s/step - loss: 0.7294 - acc: 0.7731 - val_loss: 1.3018 - val_acc: 0.6120\n",
      "Epoch 10/20\n",
      "103/104 [============================>.] - ETA: 3s - loss: 0.6965 - acc: 0.7628Epoch 00010: val_loss improved from 0.98253 to 0.93134, saving model to weights/baselineWithAugmentation400.hdf5\n",
      "104/104 [==============================] - 443s 4s/step - loss: 0.6977 - acc: 0.7617 - val_loss: 0.9313 - val_acc: 0.6906\n",
      "Epoch 11/20\n",
      "103/104 [============================>.] - ETA: 3s - loss: 0.6274 - acc: 0.7943Epoch 00011: val_loss improved from 0.93134 to 0.76559, saving model to weights/baselineWithAugmentation400.hdf5\n",
      "104/104 [==============================] - 443s 4s/step - loss: 0.6244 - acc: 0.7953 - val_loss: 0.7656 - val_acc: 0.7521\n",
      "Epoch 12/20\n",
      "103/104 [============================>.] - ETA: 3s - loss: 0.5931 - acc: 0.7967Epoch 00012: val_loss improved from 0.76559 to 0.74082, saving model to weights/baselineWithAugmentation400.hdf5\n",
      "104/104 [==============================] - 443s 4s/step - loss: 0.5926 - acc: 0.7977 - val_loss: 0.7408 - val_acc: 0.7470\n",
      "Epoch 13/20\n",
      "103/104 [============================>.] - ETA: 3s - loss: 0.5255 - acc: 0.8286Epoch 00013: val_loss did not improve\n",
      "104/104 [==============================] - 444s 4s/step - loss: 0.5238 - acc: 0.8288 - val_loss: 1.0628 - val_acc: 0.6786\n",
      "Epoch 14/20\n",
      "103/104 [============================>.] - ETA: 3s - loss: 0.5082 - acc: 0.8449Epoch 00014: val_loss did not improve\n",
      "104/104 [==============================] - 443s 4s/step - loss: 0.5123 - acc: 0.8431 - val_loss: 0.8088 - val_acc: 0.7487\n",
      "Epoch 15/20\n",
      "103/104 [============================>.] - ETA: 3s - loss: 0.4875 - acc: 0.8408Epoch 00015: val_loss improved from 0.74082 to 0.69734, saving model to weights/baselineWithAugmentation400.hdf5\n",
      "104/104 [==============================] - 443s 4s/step - loss: 0.4847 - acc: 0.8419 - val_loss: 0.6973 - val_acc: 0.8017\n",
      "Epoch 16/20\n",
      "103/104 [============================>.] - ETA: 3s - loss: 0.4359 - acc: 0.8569Epoch 00016: val_loss improved from 0.69734 to 0.67761, saving model to weights/baselineWithAugmentation400.hdf5\n",
      "104/104 [==============================] - 442s 4s/step - loss: 0.4345 - acc: 0.8573 - val_loss: 0.6776 - val_acc: 0.7863\n",
      "Epoch 17/20\n",
      "103/104 [============================>.] - ETA: 3s - loss: 0.4307 - acc: 0.8618Epoch 00017: val_loss did not improve\n",
      "104/104 [==============================] - 441s 4s/step - loss: 0.4298 - acc: 0.8622 - val_loss: 0.7878 - val_acc: 0.7658\n",
      "Epoch 18/20\n",
      "103/104 [============================>.] - ETA: 3s - loss: 0.3782 - acc: 0.8806Epoch 00018: val_loss did not improve\n",
      "104/104 [==============================] - 441s 4s/step - loss: 0.3818 - acc: 0.8803 - val_loss: 0.8144 - val_acc: 0.7453\n",
      "Epoch 19/20\n",
      "103/104 [============================>.] - ETA: 3s - loss: 0.3636 - acc: 0.8811Epoch 00019: val_loss did not improve\n",
      "104/104 [==============================] - 442s 4s/step - loss: 0.3617 - acc: 0.8817 - val_loss: 0.7497 - val_acc: 0.7829\n",
      "Epoch 20/20\n",
      "103/104 [============================>.] - ETA: 3s - loss: 0.3661 - acc: 0.8791Epoch 00020: val_loss did not improve\n",
      "104/104 [==============================] - 443s 4s/step - loss: 0.3632 - acc: 0.8803 - val_loss: 0.6985 - val_acc: 0.7863\n",
      "Found 2067 images belonging to 12 classes.\n",
      "Found 585 images belonging to 12 classes.\n",
      "Epoch 1/20\n",
      "103/104 [============================>.] - ETA: 5s - loss: 2.3353 - acc: 0.2213 Epoch 00001: val_loss improved from inf to 1.96788, saving model to weights/baselineWithAugmentation500.hdf5\n",
      "104/104 [==============================] - 695s 7s/step - loss: 2.3314 - acc: 0.2216 - val_loss: 1.9679 - val_acc: 0.2855\n",
      "Epoch 2/20\n",
      "103/104 [============================>.] - ETA: 5s - loss: 1.6635 - acc: 0.4038 Epoch 00002: val_loss improved from 1.96788 to 1.34496, saving model to weights/baselineWithAugmentation500.hdf5\n",
      "104/104 [==============================] - 687s 7s/step - loss: 1.6615 - acc: 0.4057 - val_loss: 1.3450 - val_acc: 0.5179\n",
      "Epoch 3/20\n",
      "103/104 [============================>.] - ETA: 5s - loss: 1.3940 - acc: 0.5148 Epoch 00003: val_loss improved from 1.34496 to 1.18522, saving model to weights/baselineWithAugmentation500.hdf5\n",
      "104/104 [==============================] - 685s 7s/step - loss: 1.3886 - acc: 0.5161 - val_loss: 1.1852 - val_acc: 0.6085\n",
      "Epoch 4/20\n",
      "103/104 [============================>.] - ETA: 5s - loss: 1.2315 - acc: 0.5782 Epoch 00004: val_loss improved from 1.18522 to 1.10037, saving model to weights/baselineWithAugmentation500.hdf5\n",
      "104/104 [==============================] - 682s 7s/step - loss: 1.2287 - acc: 0.5788 - val_loss: 1.1004 - val_acc: 0.6632\n",
      "Epoch 5/20\n",
      "103/104 [============================>.] - ETA: 5s - loss: 1.0542 - acc: 0.6545 Epoch 00005: val_loss improved from 1.10037 to 0.96217, saving model to weights/baselineWithAugmentation500.hdf5\n",
      "104/104 [==============================] - 686s 7s/step - loss: 1.0571 - acc: 0.6535 - val_loss: 0.9622 - val_acc: 0.7179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20\n",
      "103/104 [============================>.] - ETA: 5s - loss: 0.9500 - acc: 0.6872 Epoch 00006: val_loss did not improve\n",
      "104/104 [==============================] - 684s 7s/step - loss: 0.9524 - acc: 0.6873 - val_loss: 1.3447 - val_acc: 0.5590\n",
      "Epoch 7/20\n",
      "103/104 [============================>.] - ETA: 5s - loss: 0.8589 - acc: 0.7240 Epoch 00007: val_loss improved from 0.96217 to 0.75149, saving model to weights/baselineWithAugmentation500.hdf5\n",
      "104/104 [==============================] - 682s 7s/step - loss: 0.8576 - acc: 0.7247 - val_loss: 0.7515 - val_acc: 0.7658\n",
      "Epoch 8/20\n",
      "103/104 [============================>.] - ETA: 5s - loss: 0.7561 - acc: 0.7680 Epoch 00008: val_loss did not improve\n",
      "104/104 [==============================] - 686s 7s/step - loss: 0.7550 - acc: 0.7692 - val_loss: 0.7674 - val_acc: 0.7761\n",
      "Epoch 9/20\n",
      "103/104 [============================>.] - ETA: 5s - loss: 0.6866 - acc: 0.7695 Epoch 00009: val_loss improved from 0.75149 to 0.71278, saving model to weights/baselineWithAugmentation500.hdf5\n",
      "104/104 [==============================] - 683s 7s/step - loss: 0.6862 - acc: 0.7693 - val_loss: 0.7128 - val_acc: 0.7692\n",
      "Epoch 10/20\n",
      "103/104 [============================>.] - ETA: 5s - loss: 0.6307 - acc: 0.7967 Epoch 00010: val_loss did not improve\n",
      "104/104 [==============================] - 685s 7s/step - loss: 0.6311 - acc: 0.7967 - val_loss: 0.9614 - val_acc: 0.7026\n",
      "Epoch 11/20\n",
      "103/104 [============================>.] - ETA: 5s - loss: 0.5853 - acc: 0.8184 Epoch 00011: val_loss did not improve\n",
      "104/104 [==============================] - 682s 7s/step - loss: 0.5856 - acc: 0.8178 - val_loss: 0.9434 - val_acc: 0.7145\n",
      "Epoch 12/20\n",
      "103/104 [============================>.] - ETA: 5s - loss: 0.5768 - acc: 0.8286 Epoch 00012: val_loss did not improve\n",
      "104/104 [==============================] - 682s 7s/step - loss: 0.5759 - acc: 0.8293 - val_loss: 0.8764 - val_acc: 0.7368\n",
      "Epoch 13/20\n",
      "103/104 [============================>.] - ETA: 5s - loss: 0.5185 - acc: 0.8340 Epoch 00013: val_loss did not improve\n",
      "104/104 [==============================] - 679s 7s/step - loss: 0.5176 - acc: 0.8347 - val_loss: 1.2664 - val_acc: 0.6342\n",
      "Epoch 14/20\n",
      "103/104 [============================>.] - ETA: 5s - loss: 0.4918 - acc: 0.8399 Epoch 00014: val_loss improved from 0.71278 to 0.60976, saving model to weights/baselineWithAugmentation500.hdf5\n",
      "104/104 [==============================] - 685s 7s/step - loss: 0.4912 - acc: 0.8405 - val_loss: 0.6098 - val_acc: 0.7761\n",
      "Epoch 15/20\n",
      "103/104 [============================>.] - ETA: 5s - loss: 0.4590 - acc: 0.8525 Epoch 00015: val_loss did not improve\n",
      "104/104 [==============================] - 682s 7s/step - loss: 0.4613 - acc: 0.8520 - val_loss: 0.7480 - val_acc: 0.7709\n",
      "Epoch 16/20\n",
      "103/104 [============================>.] - ETA: 5s - loss: 0.4035 - acc: 0.8743 Epoch 00016: val_loss did not improve\n",
      "104/104 [==============================] - 682s 7s/step - loss: 0.4033 - acc: 0.8740 - val_loss: 0.7907 - val_acc: 0.7658\n",
      "Epoch 17/20\n",
      "103/104 [============================>.] - ETA: 5s - loss: 0.4042 - acc: 0.8806 Epoch 00017: val_loss did not improve\n",
      "104/104 [==============================] - 683s 7s/step - loss: 0.4007 - acc: 0.8817 - val_loss: 0.6516 - val_acc: 0.8137\n",
      "Epoch 18/20\n",
      "103/104 [============================>.] - ETA: 5s - loss: 0.3607 - acc: 0.8820 Epoch 00018: val_loss did not improve\n",
      "104/104 [==============================] - 682s 7s/step - loss: 0.3652 - acc: 0.8812 - val_loss: 0.9038 - val_acc: 0.7402\n",
      "Epoch 19/20\n",
      "103/104 [============================>.] - ETA: 5s - loss: 0.3779 - acc: 0.8874 Epoch 00019: val_loss did not improve\n",
      "104/104 [==============================] - 681s 7s/step - loss: 0.3768 - acc: 0.8875 - val_loss: 0.7919 - val_acc: 0.7692\n",
      "Epoch 20/20\n",
      "103/104 [============================>.] - ETA: 5s - loss: 0.3351 - acc: 0.8942 Epoch 00020: val_loss did not improve\n",
      "104/104 [==============================] - 683s 7s/step - loss: 0.3331 - acc: 0.8947 - val_loss: 0.6964 - val_acc: 0.8085\n"
     ]
    }
   ],
   "source": [
    "# testing results with different input sizes.\n",
    "sizes=((200,200,3),(250,250,3),(300,300,3),(400,400,3),(500,500,3))\n",
    "for size in sizes:\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, 3, padding='same', input_shape=(size), activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(64, 3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(12))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath='weights/baselineWithAugmentation{}.hdf5'.format(size[0]), verbose=1, save_best_only=True)\n",
    "\n",
    "    model.fit_generator(trainGenerator.flow_from_directory(trainDir, target_size=size[:2], batch_size=20), epochs=20,\n",
    "                       validation_data=testGenerator.flow_from_directory(valDir, target_size=size[:2], batch_size=20), callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 250 x 250 has best results\n",
    "size = (250,250,3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, 3, padding='same', input_shape=(size), activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(32, 3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(64, 3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(12))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights/baselineWithAugmentation{}.hdf5'.format(size[0]), verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit_generator(trainGenerator.flow_from_directory(trainDir, target_size=size[:2], batch_size=20), epochs=20,\n",
    "                   validation_data=testGenerator.flow_from_directory(valDir, target_size=size[:2], batch_size=20), callbacks=[checkpointer], verbose=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "219062272/219055592 [==============================] - 39s 0us/step\n",
      "Found 2067 images belonging to 12 classes.\n",
      "Found 585 images belonging to 12 classes.\n",
      "Epoch 1/12\n",
      "103/104 [============================>.] - ETA: 14s - loss: 2.6699 - acc: 0.1716Epoch 00001: val_loss improved from inf to 1.70923, saving model to weights/IRmodelFT299.hdf5\n",
      "104/104 [==============================] - 1951s 19s/step - loss: 2.6623 - acc: 0.1738 - val_loss: 1.7092 - val_acc: 0.3863\n",
      "Epoch 2/12\n",
      "103/104 [============================>.] - ETA: 14s - loss: 1.8135 - acc: 0.3859Epoch 00002: val_loss improved from 1.70923 to 1.08849, saving model to weights/IRmodelFT299.hdf5\n",
      "104/104 [==============================] - 1911s 18s/step - loss: 1.8066 - acc: 0.3884 - val_loss: 1.0885 - val_acc: 0.6462\n",
      "Epoch 3/12\n",
      "103/104 [============================>.] - ETA: 14s - loss: 1.2666 - acc: 0.5774Epoch 00003: val_loss improved from 1.08849 to 0.81271, saving model to weights/IRmodelFT299.hdf5\n",
      "104/104 [==============================] - 1915s 18s/step - loss: 1.2650 - acc: 0.5776 - val_loss: 0.8127 - val_acc: 0.7333\n",
      "Epoch 4/12\n",
      "103/104 [============================>.] - ETA: 14s - loss: 0.9181 - acc: 0.7049Epoch 00004: val_loss improved from 0.81271 to 0.63480, saving model to weights/IRmodelFT299.hdf5\n",
      "104/104 [==============================] - 1917s 18s/step - loss: 0.9190 - acc: 0.7034 - val_loss: 0.6348 - val_acc: 0.7949\n",
      "Epoch 5/12\n",
      "103/104 [============================>.] - ETA: 14s - loss: 0.7094 - acc: 0.7559Epoch 00005: val_loss improved from 0.63480 to 0.58526, saving model to weights/IRmodelFT299.hdf5\n",
      "104/104 [==============================] - 1915s 18s/step - loss: 0.7134 - acc: 0.7549 - val_loss: 0.5853 - val_acc: 0.8017\n",
      "Epoch 6/12\n",
      "103/104 [============================>.] - ETA: 14s - loss: 0.6353 - acc: 0.7799Epoch 00006: val_loss improved from 0.58526 to 0.50975, saving model to weights/IRmodelFT299.hdf5\n",
      "104/104 [==============================] - 1916s 18s/step - loss: 0.6367 - acc: 0.7787 - val_loss: 0.5098 - val_acc: 0.8239\n",
      "Epoch 7/12\n",
      "103/104 [============================>.] - ETA: 14s - loss: 0.5145 - acc: 0.8322Epoch 00007: val_loss improved from 0.50975 to 0.46615, saving model to weights/IRmodelFT299.hdf5\n",
      "104/104 [==============================] - 1913s 18s/step - loss: 0.5134 - acc: 0.8328 - val_loss: 0.4661 - val_acc: 0.8410\n",
      "Epoch 8/12\n",
      "103/104 [============================>.] - ETA: 14s - loss: 0.4859 - acc: 0.8347Epoch 00008: val_loss improved from 0.46615 to 0.43543, saving model to weights/IRmodelFT299.hdf5\n",
      "104/104 [==============================] - 1908s 18s/step - loss: 0.4853 - acc: 0.8344 - val_loss: 0.4354 - val_acc: 0.8513\n",
      "Epoch 9/12\n",
      "103/104 [============================>.] - ETA: 14s - loss: 0.3934 - acc: 0.8551Epoch 00009: val_loss improved from 0.43543 to 0.38423, saving model to weights/IRmodelFT299.hdf5\n",
      "104/104 [==============================] - 1898s 18s/step - loss: 0.3953 - acc: 0.8541 - val_loss: 0.3842 - val_acc: 0.8650\n",
      "Epoch 10/12\n",
      "103/104 [============================>.] - ETA: 14s - loss: 0.3619 - acc: 0.8743Epoch 00010: val_loss did not improve\n",
      "104/104 [==============================] - 1885s 18s/step - loss: 0.3592 - acc: 0.8751 - val_loss: 0.4305 - val_acc: 0.8427\n",
      "Epoch 11/12\n",
      "103/104 [============================>.] - ETA: 14s - loss: 0.3328 - acc: 0.8796Epoch 00011: val_loss did not improve\n",
      "104/104 [==============================] - 1888s 18s/step - loss: 0.3323 - acc: 0.8798 - val_loss: 0.3856 - val_acc: 0.8684\n",
      "Epoch 12/12\n",
      "103/104 [============================>.] - ETA: 14s - loss: 0.3097 - acc: 0.8972"
     ]
    }
   ],
   "source": [
    "# Fine tune top layers of InceptionResNetV2, testing different sizes.\n",
    "\n",
    "sizes = ((299,299,3), (400,400,3), (500,500,3), (600,600,3))\n",
    "\n",
    "for size in sizes:\n",
    "    IRmodel = InceptionResNetV2(include_top=False, weights='imagenet', input_tensor=None, input_shape=size)\n",
    "\n",
    "    for layer in IRmodel.layers[:618]:\n",
    "       layer.trainable = False\n",
    "    for layer in IRmodel.layers[618:]:\n",
    "       layer.trainable = True\n",
    "\n",
    "    x=IRmodel.output\n",
    "    x = MaxPooling2D()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(122, activation='tanh')(x)\n",
    "    x = Dropout(.5)(x)\n",
    "    predictions = Dense(12, activation='softmax')(x)\n",
    "\n",
    "    IRmodel = Model(inputs=IRmodel.input, outputs=predictions)\n",
    "\n",
    "    IRmodel.compile(optimizer=SGD(lr=0.0005, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filepath='weights/IRmodelFT{}.hdf5'.format(size[0]), verbose=1, save_best_only=True)\n",
    "\n",
    "    IRmodel.fit_generator(trainGenerator.flow_from_directory(trainDir, target_size=size[:2], batch_size=20), epochs=12,\n",
    "                          validation_data=testGenerator.flow_from_directory(valDir, target_size=size[:2], batch_size=20), callbacks=[checkpointer], verbose=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 299 x 299 seems optimal.\n",
    "\n",
    "size = (299,299,3)\n",
    "IRmodel = InceptionResNetV2(include_top=False, weights='imagenet', input_tensor=None, input_shape=size)\n",
    "\n",
    "for layer in IRmodel.layers[:618]:\n",
    "   layer.trainable = False\n",
    "for layer in IRmodel.layers[618:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "x=IRmodel.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(122, activation='tanh')(x)\n",
    "x = Dropout(.5)(x)\n",
    "predictions = Dense(12, activation='softmax')(x)\n",
    "\n",
    "IRmodel = Model(inputs=IRmodel.input, outputs=predictions)\n",
    "\n",
    "IRmodel.compile(optimizer=SGD(lr=0.0005, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights/IRmodelGAPFTall{}.hdf5'.format(size[0]), verbose=1, save_best_only=True)\n",
    "\n",
    "IRmodel.fit_generator(trainGenerator.flow_from_directory(trainDir, target_size=size[:2], batch_size=20), epochs=20,\n",
    "                      validation_data=testGenerator.flow_from_directory(valDir, target_size=size[:2], batch_size=20),\n",
    "                      callbacks=[checkpointer], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 794/794 [00:10<00:00, 74.70it/s]\n"
     ]
    }
   ],
   "source": [
    "test=[]\n",
    "for file in os.listdir(testDir):\n",
    "    test.append(['test/{}'.format(file), file])\n",
    "test = pandas.DataFrame(test, columns=[\"path\", \"file\"])\n",
    "def maketensor(path):\n",
    "    img = image.load_img(path, target_size=(299, 299))\n",
    "    x = image.img_to_array(img)\n",
    "    return numpy.expand_dims(x, axis=0)\n",
    "def maketensors(paths):\n",
    "    tensorlist = [maketensor(path) for path in tqdm(paths)]\n",
    "    return numpy.vstack(tensorlist)\n",
    "testTensors = maketensors(test['path']).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRmodel.load_weights('weights/IRmodelGAPFTall{}.hdf5'.format(size[0]))\n",
    "\n",
    "seedlingIndexes = [numpy.argmax(IRmodel.predict(numpy.expand_dims(tensor, axis=0))) for tensor in testTensors]\n",
    "seedlingPredictions=[categories[index] for index in seedlingIndexes]\n",
    "submission=pandas.DataFrame({'file':test['file'],'species': seedlingPredictions}, columns=[\"file\", \"species\"])\n",
    "submission.to_csv('/home/joel/Documents/KaggleComps/SeedlingId/IRmodelGAPFTall{}Submission.csv'.format(size[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***This model acheived 93% accuracy on test set***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2067 images belonging to 12 classes.\n",
      "Found 585 images belonging to 12 classes.\n",
      "Epoch 1/18\n",
      "158/159 [============================>.] - ETA: 16s - loss: 2.2082 - acc: 0.2522Epoch 00001: val_loss improved from inf to 1.65637, saving model to weights/model2.hdf5\n",
      "159/159 [==============================] - 3342s 21s/step - loss: 2.2077 - acc: 0.2521 - val_loss: 1.6564 - val_acc: 0.4650\n",
      "Epoch 2/18\n",
      "158/159 [============================>.] - ETA: 16s - loss: 1.4674 - acc: 0.5511Epoch 00002: val_loss improved from 1.65637 to 1.09162, saving model to weights/model2.hdf5\n",
      "159/159 [==============================] - 3289s 21s/step - loss: 1.4682 - acc: 0.5506 - val_loss: 1.0916 - val_acc: 0.6906\n",
      "Epoch 3/18\n",
      "158/159 [============================>.] - ETA: 16s - loss: 1.0495 - acc: 0.6811Epoch 00003: val_loss improved from 1.09162 to 0.79161, saving model to weights/model2.hdf5\n",
      "159/159 [==============================] - 3290s 21s/step - loss: 1.0483 - acc: 0.6812 - val_loss: 0.7916 - val_acc: 0.7778\n",
      "Epoch 4/18\n",
      "158/159 [============================>.] - ETA: 16s - loss: 0.8037 - acc: 0.7488Epoch 00004: val_loss improved from 0.79161 to 0.63380, saving model to weights/model2.hdf5\n",
      "159/159 [==============================] - 3288s 21s/step - loss: 0.8036 - acc: 0.7484 - val_loss: 0.6338 - val_acc: 0.8085\n",
      "Epoch 5/18\n",
      "158/159 [============================>.] - ETA: 16s - loss: 0.6554 - acc: 0.7902Epoch 00005: val_loss improved from 0.63380 to 0.54398, saving model to weights/model2.hdf5\n",
      "159/159 [==============================] - 3290s 21s/step - loss: 0.6550 - acc: 0.7900 - val_loss: 0.5440 - val_acc: 0.8376\n",
      "Epoch 6/18\n",
      "158/159 [============================>.] - ETA: 16s - loss: 0.5495 - acc: 0.8277Epoch 00006: val_loss improved from 0.54398 to 0.48123, saving model to weights/model2.hdf5\n",
      "159/159 [==============================] - 3293s 21s/step - loss: 0.5494 - acc: 0.8273 - val_loss: 0.4812 - val_acc: 0.8530\n",
      "Epoch 7/18\n",
      "158/159 [============================>.] - ETA: 16s - loss: 0.4873 - acc: 0.8325Epoch 00007: val_loss improved from 0.48123 to 0.43751, saving model to weights/model2.hdf5\n",
      "159/159 [==============================] - 3289s 21s/step - loss: 0.4891 - acc: 0.8321 - val_loss: 0.4375 - val_acc: 0.8650\n",
      "Epoch 8/18\n",
      "158/159 [============================>.] - ETA: 16s - loss: 0.4248 - acc: 0.8603Epoch 00008: val_loss improved from 0.43751 to 0.43533, saving model to weights/model2.hdf5\n",
      "159/159 [==============================] - 3290s 21s/step - loss: 0.4248 - acc: 0.8602 - val_loss: 0.4353 - val_acc: 0.8718\n",
      "Epoch 9/18\n",
      "158/159 [============================>.] - ETA: 16s - loss: 0.4126 - acc: 0.8622Epoch 00009: val_loss improved from 0.43533 to 0.40844, saving model to weights/model2.hdf5\n",
      "159/159 [==============================] - 3291s 21s/step - loss: 0.4116 - acc: 0.8626 - val_loss: 0.4084 - val_acc: 0.8735\n",
      "Epoch 10/18\n",
      "158/159 [============================>.] - ETA: 16s - loss: 0.3725 - acc: 0.8739Epoch 00010: val_loss improved from 0.40844 to 0.38431, saving model to weights/model2.hdf5\n",
      "159/159 [==============================] - 3290s 21s/step - loss: 0.3726 - acc: 0.8732 - val_loss: 0.3843 - val_acc: 0.8735\n",
      "Epoch 11/18\n",
      "158/159 [============================>.] - ETA: 16s - loss: 0.3240 - acc: 0.8895Epoch 00011: val_loss improved from 0.38431 to 0.36576, saving model to weights/model2.hdf5\n",
      "159/159 [==============================] - 3287s 21s/step - loss: 0.3248 - acc: 0.8892 - val_loss: 0.3658 - val_acc: 0.8889\n",
      "Epoch 12/18\n",
      "158/159 [============================>.] - ETA: 16s - loss: 0.3128 - acc: 0.8919Epoch 00012: val_loss did not improve\n",
      "159/159 [==============================] - 3287s 21s/step - loss: 0.3117 - acc: 0.8926 - val_loss: 0.3804 - val_acc: 0.8838\n",
      "Epoch 13/18\n",
      "158/159 [============================>.] - ETA: 16s - loss: 0.3120 - acc: 0.8890Epoch 00013: val_loss improved from 0.36576 to 0.35456, saving model to weights/model2.hdf5\n",
      "159/159 [==============================] - 3291s 21s/step - loss: 0.3113 - acc: 0.8892 - val_loss: 0.3546 - val_acc: 0.8889\n",
      "Epoch 14/18\n",
      "158/159 [============================>.] - ETA: 16s - loss: 0.2745 - acc: 0.9090Epoch 00014: val_loss improved from 0.35456 to 0.33385, saving model to weights/model2.hdf5\n",
      "159/159 [==============================] - 3290s 21s/step - loss: 0.2757 - acc: 0.9090 - val_loss: 0.3338 - val_acc: 0.9077\n",
      "Epoch 15/18\n",
      "158/159 [============================>.] - ETA: 16s - loss: 0.2721 - acc: 0.9085Epoch 00015: val_loss improved from 0.33385 to 0.33265, saving model to weights/model2.hdf5\n",
      "159/159 [==============================] - 3289s 21s/step - loss: 0.2715 - acc: 0.9086 - val_loss: 0.3326 - val_acc: 0.8974\n",
      "Epoch 16/18\n",
      "158/159 [============================>.] - ETA: 16s - loss: 0.2363 - acc: 0.9236Epoch 00016: val_loss did not improve\n",
      "159/159 [==============================] - 3286s 21s/step - loss: 0.2367 - acc: 0.9231 - val_loss: 0.3412 - val_acc: 0.8906\n",
      "Epoch 17/18\n",
      "158/159 [============================>.] - ETA: 16s - loss: 0.2282 - acc: 0.9216Epoch 00017: val_loss improved from 0.33265 to 0.30279, saving model to weights/model2.hdf5\n",
      "159/159 [==============================] - 3290s 21s/step - loss: 0.2297 - acc: 0.9207 - val_loss: 0.3028 - val_acc: 0.9094\n",
      "Epoch 18/18\n",
      "158/159 [============================>.] - ETA: 16s - loss: 0.2164 - acc: 0.9255Epoch 00018: val_loss did not improve\n",
      "159/159 [==============================] - 3287s 21s/step - loss: 0.2160 - acc: 0.9260 - val_loss: 0.3102 - val_acc: 0.9162\n",
      "Found 2067 images belonging to 12 classes.\n",
      "Found 585 images belonging to 12 classes.\n",
      "Epoch 1/18\n",
      "147/148 [============================>.] - ETA: 17s - loss: 2.2532 - acc: 0.2525Epoch 00001: val_loss improved from inf to 1.75165, saving model to weights/model3.hdf5\n",
      "148/148 [==============================] - 3420s 23s/step - loss: 2.2493 - acc: 0.2542 - val_loss: 1.7517 - val_acc: 0.4752\n",
      "Epoch 2/18\n",
      "147/148 [============================>.] - ETA: 17s - loss: 1.5603 - acc: 0.5211Epoch 00002: val_loss improved from 1.75165 to 1.17750, saving model to weights/model3.hdf5\n",
      "148/148 [==============================] - 3322s 22s/step - loss: 1.5559 - acc: 0.5233 - val_loss: 1.1775 - val_acc: 0.6530\n",
      "Epoch 3/18\n",
      "147/148 [============================>.] - ETA: 17s - loss: 1.1004 - acc: 0.6770Epoch 00003: val_loss improved from 1.17750 to 0.83840, saving model to weights/model3.hdf5\n",
      "148/148 [==============================] - 3324s 22s/step - loss: 1.1012 - acc: 0.6758 - val_loss: 0.8384 - val_acc: 0.7607\n",
      "Epoch 4/18\n",
      "147/148 [============================>.] - ETA: 17s - loss: 0.8415 - acc: 0.7467Epoch 00004: val_loss improved from 0.83840 to 0.66325, saving model to weights/model3.hdf5\n",
      "148/148 [==============================] - 3330s 22s/step - loss: 0.8427 - acc: 0.7465 - val_loss: 0.6632 - val_acc: 0.8017\n",
      "Epoch 5/18\n",
      "147/148 [============================>.] - ETA: 17s - loss: 0.7091 - acc: 0.7791Epoch 00005: val_loss improved from 0.66325 to 0.54516, saving model to weights/model3.hdf5\n",
      "148/148 [==============================] - 3326s 22s/step - loss: 0.7092 - acc: 0.7796 - val_loss: 0.5452 - val_acc: 0.8274\n",
      "Epoch 6/18\n",
      "147/148 [============================>.] - ETA: 17s - loss: 0.5787 - acc: 0.8172Epoch 00006: val_loss improved from 0.54516 to 0.47476, saving model to weights/model3.hdf5\n",
      "148/148 [==============================] - 3325s 22s/step - loss: 0.5789 - acc: 0.8175 - val_loss: 0.4748 - val_acc: 0.8513\n",
      "Epoch 7/18\n",
      "147/148 [============================>.] - ETA: 17s - loss: 0.5125 - acc: 0.8324Epoch 00007: val_loss improved from 0.47476 to 0.43263, saving model to weights/model3.hdf5\n",
      "148/148 [==============================] - 3316s 22s/step - loss: 0.5119 - acc: 0.8325 - val_loss: 0.4326 - val_acc: 0.8598\n",
      "Epoch 8/18\n",
      "147/148 [============================>.] - ETA: 17s - loss: 0.4525 - acc: 0.8554Epoch 00008: val_loss improved from 0.43263 to 0.40125, saving model to weights/model3.hdf5\n",
      "148/148 [==============================] - 3320s 22s/step - loss: 0.4507 - acc: 0.8563 - val_loss: 0.4012 - val_acc: 0.8684\n",
      "Epoch 9/18\n",
      "147/148 [============================>.] - ETA: 17s - loss: 0.4231 - acc: 0.8601Epoch 00009: val_loss improved from 0.40125 to 0.38595, saving model to weights/model3.hdf5\n",
      "148/148 [==============================] - 3320s 22s/step - loss: 0.4213 - acc: 0.8605 - val_loss: 0.3860 - val_acc: 0.8803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/18\n",
      "147/148 [============================>.] - ETA: 17s - loss: 0.3901 - acc: 0.8702Epoch 00010: val_loss improved from 0.38595 to 0.35180, saving model to weights/model3.hdf5\n",
      "148/148 [==============================] - 3320s 22s/step - loss: 0.3890 - acc: 0.8706 - val_loss: 0.3518 - val_acc: 0.8872\n",
      "Epoch 11/18\n",
      "147/148 [============================>.] - ETA: 17s - loss: 0.3424 - acc: 0.8870Epoch 00011: val_loss improved from 0.35180 to 0.34506, saving model to weights/model3.hdf5\n",
      "148/148 [==============================] - 3311s 22s/step - loss: 0.3427 - acc: 0.8868 - val_loss: 0.3451 - val_acc: 0.8803\n",
      "Epoch 12/18\n",
      "147/148 [============================>.] - ETA: 17s - loss: 0.3174 - acc: 0.8906Epoch 00012: val_loss improved from 0.34506 to 0.34367, saving model to weights/model3.hdf5\n",
      "148/148 [==============================] - 3312s 22s/step - loss: 0.3164 - acc: 0.8914 - val_loss: 0.3437 - val_acc: 0.8855\n",
      "Epoch 13/18\n",
      "147/148 [============================>.] - ETA: 17s - loss: 0.3171 - acc: 0.8909Epoch 00013: val_loss improved from 0.34367 to 0.34283, saving model to weights/model3.hdf5\n",
      "148/148 [==============================] - 3313s 22s/step - loss: 0.3180 - acc: 0.8902 - val_loss: 0.3428 - val_acc: 0.8838\n",
      "Epoch 14/18\n",
      "147/148 [============================>.] - ETA: 17s - loss: 0.2952 - acc: 0.8931Epoch 00014: val_loss improved from 0.34283 to 0.32439, saving model to weights/model3.hdf5\n",
      "148/148 [==============================] - 3314s 22s/step - loss: 0.2941 - acc: 0.8938 - val_loss: 0.3244 - val_acc: 0.8940\n",
      "Epoch 15/18\n",
      "147/148 [============================>.] - ETA: 17s - loss: 0.2781 - acc: 0.9028Epoch 00015: val_loss improved from 0.32439 to 0.32324, saving model to weights/model3.hdf5\n",
      "148/148 [==============================] - 3308s 22s/step - loss: 0.2770 - acc: 0.9035 - val_loss: 0.3232 - val_acc: 0.8855\n",
      "Epoch 16/18\n",
      "147/148 [============================>.] - ETA: 17s - loss: 0.2377 - acc: 0.9174Epoch 00016: val_loss improved from 0.32324 to 0.29913, saving model to weights/model3.hdf5\n",
      "148/148 [==============================] - 3309s 22s/step - loss: 0.2372 - acc: 0.9175 - val_loss: 0.2991 - val_acc: 0.8923\n",
      "Epoch 17/18\n",
      "147/148 [============================>.] - ETA: 17s - loss: 0.2284 - acc: 0.9210Epoch 00017: val_loss did not improve\n",
      "148/148 [==============================] - 3309s 22s/step - loss: 0.2305 - acc: 0.9201 - val_loss: 0.3019 - val_acc: 0.8923\n",
      "Epoch 18/18\n",
      "147/148 [============================>.] - ETA: 17s - loss: 0.2373 - acc: 0.9193Epoch 00018: val_loss did not improve\n",
      "148/148 [==============================] - 3314s 22s/step - loss: 0.2361 - acc: 0.9198 - val_loss: 0.3010 - val_acc: 0.9026\n",
      "Found 2067 images belonging to 12 classes.\n",
      "Found 585 images belonging to 12 classes.\n",
      "Epoch 1/18\n",
      "137/138 [============================>.] - ETA: 19s - loss: 2.3067 - acc: 0.2068Epoch 00001: val_loss improved from inf to 1.83013, saving model to weights/model4.hdf5\n",
      "138/138 [==============================] - 3491s 25s/step - loss: 2.3034 - acc: 0.2087 - val_loss: 1.8301 - val_acc: 0.4581\n",
      "Epoch 2/18\n",
      "137/138 [============================>.] - ETA: 19s - loss: 1.6198 - acc: 0.5158Epoch 00002: val_loss improved from 1.83013 to 1.26710, saving model to weights/model4.hdf5\n",
      "138/138 [==============================] - 3329s 24s/step - loss: 1.6178 - acc: 0.5169 - val_loss: 1.2671 - val_acc: 0.6342\n",
      "Epoch 3/18\n",
      "137/138 [============================>.] - ETA: 19s - loss: 1.1653 - acc: 0.6579Epoch 00003: val_loss improved from 1.26710 to 0.92288, saving model to weights/model4.hdf5\n",
      "138/138 [==============================] - 3326s 24s/step - loss: 1.1651 - acc: 0.6570 - val_loss: 0.9229 - val_acc: 0.7316\n",
      "Epoch 4/18\n",
      "137/138 [============================>.] - ETA: 19s - loss: 0.9225 - acc: 0.7248Epoch 00004: val_loss improved from 0.92288 to 0.74518, saving model to weights/model4.hdf5\n",
      "138/138 [==============================] - 3326s 24s/step - loss: 0.9213 - acc: 0.7258 - val_loss: 0.7452 - val_acc: 0.7880\n",
      "Epoch 5/18\n",
      "137/138 [============================>.] - ETA: 19s - loss: 0.7466 - acc: 0.7714Epoch 00005: val_loss improved from 0.74518 to 0.62450, saving model to weights/model4.hdf5\n",
      "138/138 [==============================] - 3328s 24s/step - loss: 0.7470 - acc: 0.7721 - val_loss: 0.6245 - val_acc: 0.8137\n",
      "Epoch 6/18\n",
      "137/138 [============================>.] - ETA: 19s - loss: 0.6156 - acc: 0.8129Epoch 00006: val_loss improved from 0.62450 to 0.54447, saving model to weights/model4.hdf5\n",
      "138/138 [==============================] - 3326s 24s/step - loss: 0.6151 - acc: 0.8123 - val_loss: 0.5445 - val_acc: 0.8342\n",
      "Epoch 7/18\n",
      "137/138 [============================>.] - ETA: 19s - loss: 0.5709 - acc: 0.8192Epoch 00007: val_loss improved from 0.54447 to 0.49748, saving model to weights/model4.hdf5\n",
      "138/138 [==============================] - 3330s 24s/step - loss: 0.5697 - acc: 0.8196 - val_loss: 0.4975 - val_acc: 0.8427\n",
      "Epoch 8/18\n",
      "137/138 [============================>.] - ETA: 19s - loss: 0.4925 - acc: 0.8530Epoch 00008: val_loss improved from 0.49748 to 0.44504, saving model to weights/model4.hdf5\n",
      "138/138 [==============================] - 3328s 24s/step - loss: 0.4942 - acc: 0.8522 - val_loss: 0.4450 - val_acc: 0.8547\n",
      "Epoch 9/18\n",
      "137/138 [============================>.] - ETA: 19s - loss: 0.4375 - acc: 0.8585Epoch 00009: val_loss improved from 0.44504 to 0.39650, saving model to weights/model4.hdf5\n",
      "138/138 [==============================] - 3328s 24s/step - loss: 0.4364 - acc: 0.8591 - val_loss: 0.3965 - val_acc: 0.8667\n",
      "Epoch 10/18\n",
      "137/138 [============================>.] - ETA: 19s - loss: 0.4188 - acc: 0.8713Epoch 00010: val_loss improved from 0.39650 to 0.38289, saving model to weights/model4.hdf5\n",
      "138/138 [==============================] - 3326s 24s/step - loss: 0.4181 - acc: 0.8717 - val_loss: 0.3829 - val_acc: 0.8718\n",
      "Epoch 11/18\n",
      "137/138 [============================>.] - ETA: 19s - loss: 0.3735 - acc: 0.8808Epoch 00011: val_loss improved from 0.38289 to 0.36211, saving model to weights/model4.hdf5\n",
      "138/138 [==============================] - 3344s 24s/step - loss: 0.3740 - acc: 0.8807 - val_loss: 0.3621 - val_acc: 0.8769\n",
      "Epoch 12/18\n",
      "137/138 [============================>.] - ETA: 19s - loss: 0.3229 - acc: 0.8918Epoch 00012: val_loss improved from 0.36211 to 0.35863, saving model to weights/model4.hdf5\n",
      "138/138 [==============================] - 3327s 24s/step - loss: 0.3224 - acc: 0.8921 - val_loss: 0.3586 - val_acc: 0.8838\n",
      "Epoch 13/18\n",
      "137/138 [============================>.] - ETA: 19s - loss: 0.3344 - acc: 0.8811Epoch 00013: val_loss improved from 0.35863 to 0.33897, saving model to weights/model4.hdf5\n",
      "138/138 [==============================] - 3323s 24s/step - loss: 0.3329 - acc: 0.8815 - val_loss: 0.3390 - val_acc: 0.8821\n",
      "Epoch 14/18\n",
      "137/138 [============================>.] - ETA: 19s - loss: 0.2988 - acc: 0.8972Epoch 00014: val_loss did not improve\n",
      "138/138 [==============================] - 3321s 24s/step - loss: 0.2970 - acc: 0.8979 - val_loss: 0.3505 - val_acc: 0.8803\n",
      "Epoch 15/18\n",
      "137/138 [============================>.] - ETA: 19s - loss: 0.2773 - acc: 0.9060Epoch 00015: val_loss improved from 0.33897 to 0.33833, saving model to weights/model4.hdf5\n",
      "138/138 [==============================] - 3328s 24s/step - loss: 0.2782 - acc: 0.9047 - val_loss: 0.3383 - val_acc: 0.8752\n",
      "Epoch 16/18\n",
      "137/138 [============================>.] - ETA: 19s - loss: 0.2786 - acc: 0.9062Epoch 00016: val_loss improved from 0.33833 to 0.32780, saving model to weights/model4.hdf5\n",
      "138/138 [==============================] - 3324s 24s/step - loss: 0.2782 - acc: 0.9064 - val_loss: 0.3278 - val_acc: 0.8821\n",
      "Epoch 17/18\n",
      "137/138 [============================>.] - ETA: 19s - loss: 0.2444 - acc: 0.9219Epoch 00017: val_loss did not improve\n",
      "138/138 [==============================] - 3326s 24s/step - loss: 0.2448 - acc: 0.9220 - val_loss: 0.3331 - val_acc: 0.8821\n",
      "Epoch 18/18\n",
      "137/138 [============================>.] - ETA: 19s - loss: 0.2479 - acc: 0.9201Epoch 00018: val_loss improved from 0.32780 to 0.31508, saving model to weights/model4.hdf5\n",
      "138/138 [==============================] - 3329s 24s/step - loss: 0.2471 - acc: 0.9207 - val_loss: 0.3151 - val_acc: 0.8923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 794/794 [00:10<00:00, 73.42it/s]\n",
      "/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'argmax' is deprecated. Use 'idxmax' instead. The behavior of 'argmax' will be corrected to return the positional maximum in the future. Use 'series.values.argmax' to get the position of the maximum now.\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"First attempt at building an ensemble. Trained five seperate models and saved their predictions, then\n",
    "manually averaged them.\"\"\"\n",
    "\n",
    "size = (299,299,3)\n",
    "\n",
    "for modelnum in range(5):\n",
    "    batch = 11+modelnum\n",
    "    IRmodel = InceptionResNetV2(include_top=False, weights='imagenet', input_tensor=None, input_shape=size)\n",
    "\n",
    "    for layer in IRmodel.layers[:618]:\n",
    "       layer.trainable = False\n",
    "    for layer in IRmodel.layers[618:]:\n",
    "       layer.trainable = True\n",
    "\n",
    "    x = IRmodel.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(122, activation='tanh')(x)\n",
    "    x = Dropout(.5)(x)\n",
    "    predictions = Dense(12, activation='softmax')(x)\n",
    "\n",
    "    IRmodel = Model(inputs=IRmodel.input, outputs=predictions)\n",
    "\n",
    "    IRmodel.compile(optimizer=SGD(lr=0.0005, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath='weights/model{}.hdf5'.format(modelnum), verbose=1, save_best_only=True)\n",
    "\n",
    "    IRmodel.fit_generator(trainGenerator.flow_from_directory(trainDir, target_size=size[:2], batch_size=batch), epochs=18,\n",
    "                          validation_data=testGenerator.flow_from_directory(valDir, target_size=size[:2], batch_size=batch),\n",
    "                          callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "test=[]\n",
    "for file in os.listdir(testDir):\n",
    "    test.append(['test/{}'.format(file), file])\n",
    "test = pandas.DataFrame(test, columns=[\"path\", \"file\"])\n",
    "def maketensor(path):\n",
    "    img = image.load_img(path, target_size=(299, 299))\n",
    "    x = image.img_to_array(img)\n",
    "    return numpy.expand_dims(x, axis=0)\n",
    "def maketensors(paths):\n",
    "    tensorlist = [maketensor(path) for path in tqdm(paths)]\n",
    "    return numpy.vstack(tensorlist)\n",
    "testTensors = maketensors(test['path']).astype('float32')/255\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "for number in range(5):\n",
    "    IRmodel.load_weights('weights/model{}.hdf5'.format(number))\n",
    "\n",
    "    output = []\n",
    "    for tensor in testTensors:\n",
    "        output.append(numpy.ndarray.tolist(IRmodel.predict(numpy.expand_dims(tensor, axis=0)))[0])\n",
    "    seedlingIndexes = [numpy.argmax(image) for image in output]\n",
    "    seedlingPredictions=[categories[index] for index in seedlingIndexes]\n",
    "    submission=pandas.DataFrame({'file':test['file'],'species': seedlingPredictions}, columns=[\"file\", \"species\"])\n",
    "    submission.to_csv('/home/joel/Documents/KaggleComps/SeedlingId/ensemble/model{}Submission.csv'.format(number))\n",
    "    predictions[number]=output\n",
    "\n",
    "pred=pandas.DataFrame.from_dict(predictions, 'index')\n",
    "invert = pred.to_dict()\n",
    "\n",
    "indexes=[]\n",
    "\n",
    "for image in invert:\n",
    "    frame=pandas.DataFrame(invert[image]).transpose()\n",
    "    indexes.append(numpy.argmax(frame.mean()))\n",
    "\n",
    "seedlingPredictions=[categories[index] for index in indexes]\n",
    "submission=pandas.DataFrame({'file':test['file'],'species': seedlingPredictions}, columns=[\"file\", \"species\"])\n",
    "submission.to_csv('/home/joel/Documents/KaggleComps/SeedlingId/ensemble/ensembleSubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0.000163\n",
      "1     0.992824\n",
      "2     0.000445\n",
      "3     0.000085\n",
      "4     0.000071\n",
      "5     0.000485\n",
      "6     0.000073\n",
      "7     0.003916\n",
      "8     0.000098\n",
      "9     0.001170\n",
      "10    0.000598\n",
      "11    0.000073\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "frame=pandas.DataFrame(invert[1]).transpose()\n",
    "print(frame.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Second attempt at ensemble. Feeds five InceptionResNetV2 models directly into an average, all in one model.\"\"\"\n",
    "size = (299,299,3)\n",
    "\n",
    "image = Input(size)\n",
    "\n",
    "model0 = InceptionResNetV2(include_top=False, weights='imagenet', input_tensor=image, input_shape=size)\n",
    "x0 = model0.output\n",
    "x0 = GlobalAveragePooling2D()(x0)\n",
    "x0 = Dense(122, activation='tanh')(x0)\n",
    "x0 = Dropout(.5)(x0)\n",
    "predictions0 = Dense(12, activation='softmax')(x0)\n",
    "\n",
    "model0 = Model(inputs=image, outputs=predictions0)\n",
    "\n",
    "for layer in model0.layers[:618]:\n",
    "   layer.trainable = False\n",
    "for layer in model0.layers[618:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "model0.load_weights('weights/model0.hdf5')\n",
    "\n",
    "model1 = InceptionResNetV2(include_top=False, weights='imagenet', input_tensor=image, input_shape=size)\n",
    "x1 = model1.output\n",
    "x1 = GlobalAveragePooling2D()(x1)\n",
    "x1 = Dense(122, activation='tanh')(x1)\n",
    "x1 = Dropout(.5)(x1)\n",
    "predictions1 = Dense(12, activation='softmax')(x1)\n",
    "\n",
    "model1 = Model(inputs=image, outputs=predictions1)\n",
    "\n",
    "for layer in model1.layers:\n",
    "    layer.name='{}_1'.format(layer.name)\n",
    "\n",
    "for layer in model1.layers[:618]:\n",
    "    layer.trainable = False\n",
    "for layer in model1.layers[618:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model1.load_weights('weights/model1.hdf5')\n",
    "\n",
    "model2 = InceptionResNetV2(include_top=False, weights='imagenet', input_tensor=image, input_shape=size)\n",
    "x2 = model2.output\n",
    "x2 = GlobalAveragePooling2D()(x2)\n",
    "x2 = Dense(122, activation='tanh')(x2)\n",
    "x2 = Dropout(.5)(x2)\n",
    "predictions2 = Dense(12, activation='softmax')(x2)\n",
    "\n",
    "model2 = Model(inputs=image, outputs=predictions2)\n",
    "\n",
    "for layer in model2.layers:\n",
    "    layer.name='{}_2'.format(layer.name)\n",
    "\n",
    "for layer in model2.layers[:618]:\n",
    "   layer.trainable = False\n",
    "for layer in model2.layers[618:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "model2.load_weights('weights/model2.hdf5')\n",
    "\n",
    "model3 = InceptionResNetV2(include_top=False, weights='imagenet', input_tensor=image, input_shape=size)\n",
    "x3 = model3.output\n",
    "x3 = GlobalAveragePooling2D()(x3)\n",
    "x3 = Dense(122, activation='tanh')(x3)\n",
    "x3 = Dropout(.5)(x3)\n",
    "predictions3 = Dense(12, activation='softmax')(x3)\n",
    "\n",
    "model3 = Model(inputs=image, outputs=predictions3)\n",
    "\n",
    "for layer in model3.layers:\n",
    "    layer.name='{}_3'.format(layer.name)\n",
    "\n",
    "for layer in model3.layers[:618]:\n",
    "   layer.trainable = False\n",
    "for layer in model3.layers[618:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "model3.load_weights('weights/model3.hdf5')\n",
    "\n",
    "model4 = InceptionResNetV2(include_top=False, weights='imagenet', input_tensor=image, input_shape=size)\n",
    "x4 = model4.output\n",
    "x4 = GlobalAveragePooling2D()(x4)\n",
    "x4 = Dense(122, activation='tanh')(x4)\n",
    "x4 = Dropout(.5)(x4)\n",
    "predictions4 = Dense(12, activation='softmax')(x4)\n",
    "\n",
    "model4 = Model(inputs=image, outputs=predictions4)\n",
    "\n",
    "for layer in model4.layers:\n",
    "    layer.name='{}_4'.format(layer.name)\n",
    "\n",
    "for layer in model4.layers[:618]:\n",
    "   layer.trainable = False\n",
    "for layer in model4.layers[618:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "model4.load_weights('weights/model4.hdf5')\n",
    "\n",
    "xe = Average()([model0.output, model1.output, model2.output, model3.output, model4.output])\n",
    "\n",
    "ensemble = Model(inputs=image, outputs=xe)\n",
    "\n",
    "ensemble.compile(optimizer=SGD(lr=0.0005, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither Ensemble model out performed the final ResNetInceptionV2 model, probably because the five models were too similar. There may also be better ways of combining them than average."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
